{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbe7077-f9a4-4219-a025-1f895232dfa3",
   "metadata": {},
   "source": [
    "## Numba CUDA intro\n",
    "\n",
    "> **GITHUB: https://github.com/daniel-vera-g/numba-cuda-intro**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf86f1-0912-430f-8470-47d7a23d3b6d",
   "metadata": {},
   "source": [
    "## Short summary of CUDA components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee107744-bdf5-4761-8202-fb7f92233eed",
   "metadata": {},
   "source": [
    "1. host: CPU\n",
    "2. device: GPU\n",
    "3. host memory: system main memory\n",
    "4. device memory: onboard memory on a GPU card\n",
    "5. kernel: a GPU function launched by the host and executed on the device\n",
    "6. device function: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)\n",
    "\n",
    "![](http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/CUDA_processing_flow_%28En%29.PNG/450px-CUDA_processing_flow_%28En%29.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a88fa-dec2-4d23-bac2-d2379903c8e3",
   "metadata": {},
   "source": [
    "### CUDA structure\n",
    "\n",
    "A grid contains blocks with threads:\n",
    "\n",
    "![](https://www.researchgate.net/profile/Omar-Bouattane/publication/321666991/figure/fig2/AS:572931245260800@1513608861931/Figure-2-Execution-model-of-a-CUDA-program-on-NVidias-GPU-Hierarchy-grid-blocks-and.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca1209-4160-43ab-91e7-4b3803ac3ec2",
   "metadata": {},
   "source": [
    "## Use CUDA with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f21384-01fc-4caa-992d-b5e57c5e0f70",
   "metadata": {},
   "source": [
    "**NOTE**: If you don't have a CUDA capable GPU, you can activate the _simulation_ mode by starting Jupyterlab with the `NUMBA_ENABLE_CUDASIM=1` ENV:\n",
    "\n",
    "> `NUMBA_ENABLE_CUDASIM=1 jupyter-lab`\n",
    "\n",
    "To learn more about how to use the simulator for debugging, see the [cuda-debugger](./cuda-debugger.ipynb) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578a7fb6-6bb1-49f3-aa8c-3af83c732431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus) # -> Will probably not work, if you don't have a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16a734",
   "metadata": {},
   "source": [
    "### CUDA Python rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86908c3",
   "metadata": {},
   "source": [
    "When using the `@cuda.jit` Annotation, the Numba just in time compiler creates an optimized version of the code to be executed on the GPU.\n",
    "\n",
    "By doing this, it makes use of the [Single instruction, multiple threads](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads) concept used by CUDA. As the SIMT concept executes the code by multiple threads in parallel, often used array expression should be avoided. Otherwise the sequential nature of this operation would hinder the goal of having a preformance boost compared to the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966977d",
   "metadata": {},
   "source": [
    "### Not supported operations\n",
    "\n",
    "Due to the explanation obove, some python operations are not supported in CUDA kernel code. Some of the most important ones to be aware of are:\n",
    "\n",
    "1. **Generators** that are constructed with the `yield` statement\n",
    "2. **Comprehensions**. Like _list, dict, set or generator_ comprehensions. Those are often used in functional python programs.\n",
    "3. **Exception handling** with `try...except` or `try...finally`\n",
    "\n",
    "\n",
    "For a more detailed list, consult the [Numba documentation](https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html#constructs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf52233",
   "metadata": {},
   "source": [
    "### Support of Numpy operations\n",
    "\n",
    "By doing dynamic memory allocation, a lot of read and write operations to the global memory are done which diminishes the performance greatly. Therefore, Numba does not allow the use of dynamic memory allocation.\n",
    "\n",
    "As effect some Numpy features like array creation, array methods or functions that return a new array are not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ef43a",
   "metadata": {},
   "source": [
    "<!--- ## Table of contents -->\n",
    "\n",
    "<!--- - A crash course over the theory: [CUDA components](#Short-summary-of-CUDA-components) -->\n",
    "<!--- - Use CUDA in with Numba: [CUDA with Numba](#Use-CUDA-with-Numba) -->\n",
    "\n",
    "## Continue here...\n",
    "\n",
    "> You're here ðŸ‘‰: [Basics](./numba_cuda_tutorial.ipynb)\n",
    "\n",
    "**CUDA concepts in Numba:**\n",
    "\n",
    "- What Kernels are and how they work: [Kernels](kernels.ipynb)\n",
    "- How to manage memory when doing operations: [Memory management](memory-management.ipynb)\n",
    "- How to debug Numba code: [Debugging](./cuda-debugger.ipynb)\n",
    "- Other useful CUDA features in Numba: [Other Numba features](./other-cuda-features.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
