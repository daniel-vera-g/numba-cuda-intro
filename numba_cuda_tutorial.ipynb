{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbe7077-f9a4-4219-a025-1f895232dfa3",
   "metadata": {},
   "source": [
    "# Numba CUDA intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf86f1-0912-430f-8470-47d7a23d3b6d",
   "metadata": {},
   "source": [
    "## Short summary CUDA components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee107744-bdf5-4761-8202-fb7f92233eed",
   "metadata": {},
   "source": [
    "1. host: CPU\n",
    "2. device: GPU\n",
    "3. host memory: system main memory\n",
    "4. device memory: onboard memory on a GPU card\n",
    "5. kernel: a GPU function launched by the host and executed on the device\n",
    "6. device function: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)\n",
    "\n",
    "![](http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/CUDA_processing_flow_%28En%29.PNG/450px-CUDA_processing_flow_%28En%29.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a88fa-dec2-4d23-bac2-d2379903c8e3",
   "metadata": {},
   "source": [
    "### CUDA structure\n",
    "\n",
    "A grid contains blocks with threads:\n",
    "\n",
    "![](https://www.researchgate.net/profile/Omar-Bouattane/publication/321666991/figure/fig2/AS:572931245260800@1513608861931/Figure-2-Execution-model-of-a-CUDA-program-on-NVidias-GPU-Hierarchy-grid-blocks-and.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca1209-4160-43ab-91e7-4b3803ac3ec2",
   "metadata": {},
   "source": [
    "## Use CUDA with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f21384-01fc-4caa-992d-b5e57c5e0f70",
   "metadata": {},
   "source": [
    "**NOTE**: If you don't have a CUDA capable GPU, you can activate the _simulation_ mode by starting Jupyterlab with the `NUMBA_ENABLE_CUDASIM=1` ENV:\n",
    "\n",
    "> `NUMBA_ENABLE_CUDASIM=1 jupyter-lab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578a7fb6-6bb1-49f3-aa8c-3af83c732431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus) # -> Will probably not work, if you don't have a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0e263-7097-4646-8cc8-e5ee1c530f0b",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "> GPU function called form the CPU\n",
    "\n",
    "- Pass and get data through arrays\n",
    "- Before calling Kernel function, declare thread hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e08fc0e-fd64-4570-a844-1fc6b8f90cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our jit cuda enabled function \n",
    "@cuda.jit\n",
    "def test_kernel(an_array):\n",
    "    \"\"\"\n",
    "    TODO Code for kernel.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5efdad2-7b84-463d-9c74-3d420147f05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Declare kernel:\n",
    "\n",
    "# 1. Data array to work with\n",
    "# Docs: \"Return a new array of given shape and type, filled with ones.\"\n",
    "dataArr = np.ones(256)\n",
    "\n",
    "# 2. Define Thread values\n",
    "\n",
    "# Block size depending on size data array, shared memory, supported hardware, ...\n",
    "threads_in_block = 32\n",
    "blocks_in_grid = (dataArr.size + (threads_in_block - 1))\n",
    "\n",
    "# 3. \"Call\" Kernel\n",
    "test_kernel[blocks_in_grid, threads_in_block](dataArr)\n",
    "\n",
    "print(dataArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea35844-4268-48cc-991e-484a2f63e44b",
   "metadata": {},
   "source": [
    "### Positon of thread in grid & block\n",
    "\n",
    "> Get the position of thread by getting the information in the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95382ee-6350-4f01-99f8-a36529a7d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def position_kernel(an_array):\n",
    "    # Let's get the thread and corresponding block\n",
    "    tt = cuda.threadIdx.x # Aka. X-Dimension\n",
    "    tb = cuda.blockIdx.x # Aka. Y-Dimension\n",
    "    \n",
    "    # \"Size\" aka. width of Block: Number threads in Block\n",
    "    bs = cuda.blockDim.x\n",
    "    \n",
    "    position = tt + tb * bs\n",
    "\n",
    "    if position < an_array.size:\n",
    "        an_array[position] *= 2 # Just double size as example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b94874c-2bb6-49db-a366-5a7a9a247bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Same as above:\n",
    "dataArr = np.ones(256)\n",
    "threads_in_block = 32\n",
    "blocks_in_grid = (dataArr.size + (threads_in_block - 1))\n",
    "\n",
    "position_kernel[blocks_in_grid, threads_in_block](dataArr)\n",
    "\n",
    "# TODO understand\n",
    "print(dataArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cdc74e-bfa3-4656-b02f-e835c6f786dd",
   "metadata": {},
   "source": [
    "### Automating position search\n",
    "\n",
    "> Automate search for position using `numba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d2026b-0bad-407f-91b7-b49a60858924",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def automated_kernel(an_array):\n",
    "    # Docs: \"Return the absolute position of the current thread in the entire grid of blocks.\"\n",
    "    # `ndim` => Number dimensions\n",
    "    position = cuda.grid(1)\n",
    "        \n",
    "    # Same as above\n",
    "    if position < an_array.size:\n",
    "        an_array[position] *= 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c741723-cc44-47c1-afa6-fe9d4a3e0486",
   "metadata": {},
   "source": [
    "Add host to call Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16574445-b1bd-4008-81bf-95fb66c0334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Same as above:\n",
    "dataArr = np.ones(256)\n",
    "threads_in_block = 32\n",
    "blocks_in_grid = (dataArr.size + (threads_in_block - 1))\n",
    "\n",
    "automated_kernel[blocks_in_grid, threads_in_block](dataArr)\n",
    "\n",
    "print(dataArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f64d93-97b7-401d-8642-86fbf7e0541d",
   "metadata": {},
   "source": [
    "## Memory management\n",
    "\n",
    "> Use manual memory management to have more efficient code\n",
    "\n",
    "Example using matrix multiplication:\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Matrix_multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d0482d-2bf0-4e8f-a639-2236345876c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(inputArr1, inputArr2, outputArr):\n",
    "    \"\"\"\n",
    "    Do a Matrix multiplication: inputArr1 * inputArr2 = outputArr\n",
    "    \"\"\"\n",
    "    # Get a two dimensional grid for calculations\n",
    "    row, column = cuda.grid(2)\n",
    "    \n",
    "    # Check that we're in the boundaries\n",
    "    # & not accessing prohibited memory\n",
    "    if row < outputArr.shape[0] and column < outputArr.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(inputArr1.shape[1]):\n",
    "            tmp += inputArr1[row, k] * inputArr2[k, column]\n",
    "            outputArr[row, column] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24c35c2-09bf-45d7-91c2-32e225725cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]\n",
      " [2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352. 2352.\n",
      "  2352. 2352. 2352. 2352.]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Create 2D-Arrays filled with 7 and 8s\n",
    "# NOTE: Row <-> Column or Column <-> Row should be equal size\n",
    "inputArr1 = np.full((12, 42), 7, float)\n",
    "inputArr2 = np.full((42, 16), 8, float)\n",
    "\n",
    "# Copy 2D-Arrays to device aka \"GPU\"\n",
    "inputArr1_global_mem  = cuda.to_device(inputArr1)\n",
    "inputArr2_global_mem  = cuda.to_device(inputArr2)\n",
    "\n",
    "# Allocate mem on device for result\n",
    "# Shape = non-equal size values from above\n",
    "outputArr_global_mem = cuda.device_array((12,16))\n",
    "\n",
    "# TODO how get values about threadsperblock?\n",
    "threads_in_block = (16, 16)\n",
    "blocks_per_grid_x = int(math.ceil(inputArr1.shape[0] / threads_in_block[0]))\n",
    "blocks_per_grid_y = int(math.ceil(inputArr1.shape[1] / threads_in_block[1]))\n",
    "blocks_in_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "matmul[blocks_in_grid, threads_in_block](inputArr1_global_mem, inputArr2_global_mem, outputArr_global_mem)\n",
    "\n",
    "# Copy result back to host aka. CPU\n",
    "outputArr = outputArr_global_mem.copy_to_host()\n",
    "\n",
    "print(outputArr)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14ae99-b7f7-477c-9e33-5383ad1bac6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
